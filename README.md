# VisionAssist - Interactive AI Visual Assistant

An AI-powered real-time camera-based image captioning system with conversational voice features, designed to assist visually impaired users.

## ğŸŒŸ Features

### Core Functionality
- **Real-time Camera Captioning**: Live AI-generated descriptions of camera feed
- **Interactive Voice Conversations**: ChatGPT-style voice interactions about visual content
- **Speech Recognition**: Browser-based voice input for hands-free operation
- **Audio Feedback**: Text-to-speech for all responses and captions
- **Accessible UI**: WCAG-compliant interface optimized for screen readers

### AI Technologies
- **Vision AI**: BLIP (Bootstrapped Language-Image Pre-training) for image captioning
- **Conversational AI**: Ollama with Llama 3.2 for local, private conversations
- **Speech Processing**: Web Speech API for voice recognition and synthesis

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 14+
- Webcam/camera access
- Modern web browser (Chrome, Firefox, Edge)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/VisionAssist.git
   cd VisionAssist
   ```

2. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install Ollama** (for conversational features)
   ```bash
   # Windows
   winget install Ollama.Ollama
   
   # macOS
   brew install ollama
   
   # Linux
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

4. **Download AI model**
   ```bash
   ollama pull llama3.2
   ```

5. **Install frontend dependencies**
   ```bash
   cd visionassist-frontend
   npm install
   ```

### Running the Application

1. **Start Ollama service**
   ```bash
   ollama serve
   ```

2. **Start the conversational backend** (Terminal 1)
   ```bash
   python conversational_backend.py
   ```

3. **Start the original backend** (Terminal 2)
   ```bash
   python api_backend.py
   ```

4. **Start the frontend** (Terminal 3)
   ```bash
   cd visionassist-frontend
   npm start
   ```

5. **Open your browser** to `http://localhost:3000`

## ğŸ® How to Use

### Basic Captioning
1. Click "ğŸ“¹ Start Real-Time Captioning"
2. Allow camera permissions
3. Listen to automatic scene descriptions

### Conversational Mode
1. Start basic captioning first
2. Click "ğŸ’¬ Chat Mode" to enable conversations
3. Use voice input to ask questions:
   - "What do you see?"
   - "How many people are here?"
   - "What colors are visible?"
   - "Describe this scene in detail"
   - "Read any text you can see"

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontend â”‚    â”‚  Flask Backends  â”‚    â”‚   AI Services   â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Camera Feed   â”‚â—„â”€â”€â–ºâ”‚ â€¢ Vision API     â”‚â—„â”€â”€â–ºâ”‚ â€¢ BLIP Model    â”‚
â”‚ â€¢ Voice Input   â”‚    â”‚   (Port 5000)    â”‚    â”‚ â€¢ Transformers  â”‚
â”‚ â€¢ Speech Output â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Chat History  â”‚    â”‚ â€¢ Conversation   â”‚â—„â”€â”€â–ºâ”‚ â€¢ Ollama        â”‚
â”‚                 â”‚    â”‚   API (Port 5001)â”‚    â”‚ â€¢ Llama 3.2     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
VisionAssist/
â”œâ”€â”€ conversational_backend.py   # Ollama integration API
â”œâ”€â”€ api_backend.py              # Original vision API
â”œâ”€â”€ config.py                   # Configuration settings
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ visionassist-frontend/      # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js             # Main application
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ VoiceInput.js  # Speech recognition
â”‚   â”‚   â”‚   â””â”€â”€ VoiceInput.css # Voice UI styles
â”‚   â”‚   â””â”€â”€ App.css            # Main styles
â”‚   â””â”€â”€ package.json           # Node dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ”§ Configuration

### API Endpoints
- **Frontend**: http://localhost:3000
- **Vision API**: http://localhost:5000
- **Conversation API**: http://localhost:5001
- **Ollama**: http://localhost:11434

### Customization
Edit `config.py` to adjust:
- Model parameters
- API settings
- Performance options

## ğŸ¯ Target Users

- **Visually impaired individuals** seeking real-time scene understanding
- **Accessibility advocates** exploring AI-assisted navigation
- **Developers** interested in multimodal AI applications
- **Researchers** studying human-AI interaction patterns

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **BLIP**: Salesforce Research for the vision-language model
- **Ollama**: For local AI inference capabilities
- **Web Speech API**: For browser-based speech recognition
- **React**: For the accessible frontend framework

## ğŸ› Known Issues

- Speech recognition accuracy varies by browser and accent
- Initial model loading may take 30-60 seconds
- Camera permissions required for full functionality

## ğŸ“ Support

For questions or issues, please open a GitHub issue or contact the maintainers.

---

**VisionAssist** - Empowering independence through AI-powered visual assistance. 
